{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6847931,"sourceType":"datasetVersion","datasetId":3936750},{"sourceId":6867914,"sourceType":"datasetVersion","datasetId":3946973},{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644},{"sourceId":7026136,"sourceType":"datasetVersion","datasetId":4040811},{"sourceId":2626,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":1901}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bert Fine Tune Model","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T07:02:48.851182Z","iopub.execute_input":"2023-12-12T07:02:48.851485Z","iopub.status.idle":"2023-12-12T07:02:48.901326Z","shell.execute_reply.started":"2023-12-12T07:02:48.851460Z","shell.execute_reply":"2023-12-12T07:02:48.900005Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-512-a-8/2/saved_model.pb\n/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-512-a-8/2/keras_metadata.pb\n/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-512-a-8/2/assets/vocab.txt\n/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-512-a-8/2/variables/variables.index\n/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-512-a-8/2/variables/variables.data-00000-of-00001\n/kaggle/input/bertfinetunev2/bert_v2/config.json\n/kaggle/input/bertfinetunev2/bert_v2/tokenizer_config.json\n/kaggle/input/bertfinetunev2/bert_v2/model.safetensors\n/kaggle/input/bertfinetunev2/bert_v2/special_tokens_map.json\n/kaggle/input/bertfinetunev2/bert_v2/vocab.txt\n/kaggle/input/daigt-proper-train-dataset/train_drcat_03.csv\n/kaggle/input/daigt-proper-train-dataset/train_drcat_02.csv\n/kaggle/input/daigt-proper-train-dataset/train_drcat_04.csv\n/kaggle/input/daigt-proper-train-dataset/train_drcat_01.csv\n/kaggle/input/argugpt/argugpt.csv\n/kaggle/input/argugpt/machine-dev.csv\n/kaggle/input/argugpt/machine-test.csv\n/kaggle/input/argugpt/machine-train.csv\n/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\n/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\n/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\n/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\n/kaggle/input/daigt-external-dataset/daigt_external_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport pandas as pd\nfrom sklearn.utils import resample\n\nimport torch\nfrom transformers import DistilBertForSequenceClassification, AdamW\nfrom tqdm import tqdm\n\nfrom torch.utils.data import Dataset\nfrom transformers import DistilBertTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-12-12T07:20:50.195728Z","iopub.execute_input":"2023-12-12T07:20:50.196621Z","iopub.status.idle":"2023-12-12T07:20:53.238062Z","shell.execute_reply.started":"2023-12-12T07:20:50.196588Z","shell.execute_reply":"2023-12-12T07:20:53.237213Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(f\"Device Found : {device}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-12T07:20:36.676348Z","iopub.execute_input":"2023-12-12T07:20:36.676710Z","iopub.status.idle":"2023-12-12T07:20:39.762601Z","shell.execute_reply.started":"2023-12-12T07:20:36.676680Z","shell.execute_reply":"2023-12-12T07:20:39.761581Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Device Found : cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Dataset 1","metadata":{}},{"cell_type":"code","source":"# train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n# test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n\n# ## External Dataset 1\n# ext_df1 = pd.read_csv('/kaggle/input/daigt-external-dataset/daigt_external_dataset.csv') [['id','source_text']]\n# ext_df2 = pd.read_csv('/kaggle/input/argugpt/argugpt.csv')[['id','text']]\n\n# ext_df1.rename(columns={'source_text':'text'}, inplace=True)\n# ext_df1['generated'] = 1\n# ext_df2['generated'] = 1\n\n\n# all_train = pd.concat([ext_df2, ext_df1, train[['id','text','generated']]])\n# all_train.head(3)\n\n# df_majority = all_train[all_train['generated'] == 1]  # AI-written essays\n# df_minority = all_train[all_train['generated'] == 0]  # Human-written essays\n\n# # Upsample the minority class (human-written essays)\n# df_minority_upsampled = resample(df_minority,\n#                                  replace=True,      # sample with replacement\n#                                  n_samples=6462,    # to match majority class\n#                                  random_state=123)  # reproducible results\n\n# # Combine majority class with upsampled minority class\n# df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n\n# # Display new class counts\n# print(df_upsampled['generated'].value_counts())\n\n# train = df_upsampled.copy().reset_index(drop=True)\n# train = train.rename(columns={'generated': 'label'})\n# train.head()\n\n\n\n\n# ## Dataset 2 Code \n\n\n# # Model Path\n# model_path = '/kaggle/input/bertfinetunev2/bert_v2' \n# #model_path  = 'distilbert-base-uncased'","metadata":{"execution":{"iopub.status.busy":"2023-11-22T13:21:08.962015Z","iopub.execute_input":"2023-11-22T13:21:08.962399Z","iopub.status.idle":"2023-11-22T13:21:08.971464Z","shell.execute_reply.started":"2023-11-22T13:21:08.962333Z","shell.execute_reply":"2023-11-22T13:21:08.970589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset 2","metadata":{}},{"cell_type":"code","source":"# External Dataset 2 \next_df1 = pd.read_csv('/kaggle/input/daigt-proper-train-dataset/train_drcat_04.csv')\nprint(ext_df1['label'].value_counts())\n\ntrain = ext_df1.copy().reset_index(drop=True)\ntrain.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T07:20:59.185473Z","iopub.execute_input":"2023-12-12T07:20:59.186476Z","iopub.status.idle":"2023-12-12T07:21:01.436202Z","shell.execute_reply.started":"2023-12-12T07:20:59.186441Z","shell.execute_reply":"2023-12-12T07:21:01.435272Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"label\n0    29792\n1    14414\nName: count, dtype: int64\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       essay_id                                               text  label  \\\n0  E897534557AF   In recent years, technology has had a profoun...      1   \n1  DFBA34FFE11D  Should students participate in an extracurricu...      0   \n2      af37ecf5  The electoral college is a symbol of mockery a...      0   \n\n                 source                                             prompt  \\\n0  mistral7binstruct_v2  \\nTask: Write an essay discussing the positive...   \n1       persuade_corpus                                                NaN   \n2          train_essays                                                NaN   \n\n   fold  \n0     1  \n1     2  \n2     5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>source</th>\n      <th>prompt</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E897534557AF</td>\n      <td>In recent years, technology has had a profoun...</td>\n      <td>1</td>\n      <td>mistral7binstruct_v2</td>\n      <td>\\nTask: Write an essay discussing the positive...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DFBA34FFE11D</td>\n      <td>Should students participate in an extracurricu...</td>\n      <td>0</td>\n      <td>persuade_corpus</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>af37ecf5</td>\n      <td>The electoral college is a symbol of mockery a...</td>\n      <td>0</td>\n      <td>train_essays</td>\n      <td>NaN</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Split the DataFrame into train and test sets\n#train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\ntrain_df = train.copy()[['text','label']]\ntrain_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T07:21:02.289510Z","iopub.execute_input":"2023-12-12T07:21:02.290391Z","iopub.status.idle":"2023-12-12T07:21:02.410026Z","shell.execute_reply.started":"2023-12-12T07:21:02.290357Z","shell.execute_reply":"2023-12-12T07:21:02.409118Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0   In recent years, technology has had a profoun...      1\n1  Should students participate in an extracurricu...      0\n2  The electoral college is a symbol of mockery a...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In recent years, technology has had a profoun...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Should students participate in an extracurricu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The electoral college is a symbol of mockery a...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### DataSet Class","metadata":{}},{"cell_type":"code","source":"\nclass TextDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=512):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __getitem__(self, index):\n        text = str(self.data.text.iloc[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_token_type_ids=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        return {\n            'ids': ids.flatten(),\n            'mask': mask.flatten(),\n            'targets': torch.tensor(self.data.label.iloc[index], dtype=torch.long)\n        }\n\n    def __len__(self):\n        return self.len\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T07:21:05.430554Z","iopub.execute_input":"2023-12-12T07:21:05.430932Z","iopub.status.idle":"2023-12-12T07:21:05.439729Z","shell.execute_reply.started":"2023-12-12T07:21:05.430903Z","shell.execute_reply":"2023-12-12T07:21:05.438708Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### DataLoader ","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n#model_path = '/kaggle/input/bertfinetunev2/bert_v2' \n\nmodel_path = '/kaggle/working/fine_tune_bert_v3_10epoch'\n\ntokenizer = DistilBertTokenizer.from_pretrained(model_path)\ntrain_dataset = TextDataset(train_df, tokenizer, max_length=512) # max allowed length for bert\n#val_dataset = TextDataset(val_df, tokenizer, max_length=512)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n#val_loader = DataLoader(val_dataset, batch_size=16)\n\n\nmodel = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=2)\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\nprint(\"Model Loaded Successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T07:21:09.563884Z","iopub.execute_input":"2023-12-12T07:21:09.564718Z","iopub.status.idle":"2023-12-12T07:21:13.935640Z","shell.execute_reply.started":"2023-12-12T07:21:09.564678Z","shell.execute_reply":"2023-12-12T07:21:13.934796Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model Loaded Successfully.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Fine Tune Bert","metadata":{}},{"cell_type":"code","source":"for epoch in range(5):  # number of training epochs\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(enumerate(train_loader), desc=f'Epoch {epoch}', total=len(train_loader), leave=False, disable=False)\n\n    for batch_idx, batch in progress_bar:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n\n        model.zero_grad()\n        outputs = model(ids, attention_mask=mask, labels=targets)\n        loss = outputs[0]\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        # Update progress bar\n        progress_bar.set_postfix({'Training Loss': '{:.7f}'.format(total_loss / (batch_idx + 1))})\n\n    progress_bar.close()\n    print(f'Epoch: {epoch}, Training Loss: {total_loss / len(train_loader)}')\n    \n    # After Each Epoch Save Model \n    model.save_pretrained('/kaggle/working/fine_tune_bert_v3_10epoch')\n    tokenizer.save_pretrained('/kaggle/working/fine_tune_bert_v3_10epoch')\n\n    # Add code for validation here if needed\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T07:21:26.188215Z","iopub.execute_input":"2023-12-12T07:21:26.188943Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epoch 0:  84%|████████▎ | 1157/1382 [25:25<04:53,  1.30s/it, Training Loss=0.0099664]","output_type":"stream"}]},{"cell_type":"markdown","source":"### Save Model","metadata":{}},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/fine_tune_bert_v3_10epoch')\ntokenizer.save_pretrained('/kaggle/working/fine_tune_bert_v3_10epoch')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction with Probabilities","metadata":{}},{"cell_type":"code","source":"def predict_with_probability(model, tokenizer, texts, max_length=512):\n    model.eval()  # Put the model in evaluation mode\n\n    predictions = []\n    probabilities = []\n    with torch.no_grad():  # Disable gradient calculation for efficiency\n        for text in texts:\n            # Preprocess and tokenize the text\n            inputs = tokenizer.encode_plus(\n                text,\n                add_special_tokens=True,\n                max_length=max_length,\n                padding='max_length',\n                truncation=True,\n                return_attention_mask=True,\n                return_tensors='pt'\n            )\n            \n            ids = inputs['input_ids'].to(device)\n            mask = inputs['attention_mask'].to(device)\n\n            # Get model predictions\n            outputs = model(ids, attention_mask=mask)\n            logits = outputs.logits\n\n            # Convert logits to probabilities\n            probs = torch.nn.functional.softmax(logits, dim=1)\n\n            # Get the predicted class and its probability\n            predicted_class = torch.argmax(probs, dim=1).cpu().numpy()\n            max_prob = probs.max(dim=1).values.cpu().numpy()\n\n            predictions.append(predicted_class[0])\n            probabilities.append(max_prob[0])\n\n    return predictions, probabilities\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T01:35:25.612880Z","iopub.execute_input":"2023-11-23T01:35:25.613600Z","iopub.status.idle":"2023-11-23T01:35:25.657552Z","shell.execute_reply.started":"2023-11-23T01:35:25.613523Z","shell.execute_reply":"2023-11-23T01:35:25.656028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_texts = [\"Aaa bbb ccc.\", \"Bbb ccc ddd.\", \"CCC ddd eee.\"]\npredictions, probs = predict_with_probability(model, tokenizer, sample_texts)\n\nfor text, pred, prob in zip(sample_texts, predictions, probs):\n    print(f\"Text: {text}, Predicted Class: {'AI' if pred == 1 else 'Human'}, Probability: {prob:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T10:44:51.074288Z","iopub.execute_input":"2023-11-22T10:44:51.074641Z","iopub.status.idle":"2023-11-22T10:44:51.118467Z","shell.execute_reply.started":"2023-11-22T10:44:51.074610Z","shell.execute_reply":"2023-11-22T10:44:51.117508Z"},"trusted":true},"execution_count":null,"outputs":[]}]}